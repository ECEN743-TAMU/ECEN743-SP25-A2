{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECEN743 Spring 2025 - Assignment 2\n",
    "## Tabular RL Algorithms\n",
    "\n",
    "In this assignment, you will solve the FrozenLake-v1 environment from [Gymnasium](https://gymnasium.farama.org/). You will be using this helper file to answer questions in your assignment. \n",
    "\n",
    "**Note that you do not need to start from the scratch. Only write your code between the following lines. Do not modify other parts.**  \n",
    "\"### YOUR CODE HERE\"  \n",
    "\"### END OF YOUR CODE\"\n",
    "\n",
    "## Introduction of the FrozenLake Environment\n",
    "\n",
    "Frozen lake involves crossing a frozen lake from start to goal without falling into any holes by walking over the frozen lake. The player may not always move in the intended direction due to the slippery nature of the frozen lake. The game starts with the player at location [0,0] of the frozen lake grid world with the goal located at far extent of the world e.g. [3,3] for the 4x4 environment. Holes in the ice are distributed in set locations using a pre-determined map, and the player makes moves until they reach the goal or fall in a hole. The map is given below for your reference\n",
    "\n",
    "        SFFF\n",
    "        FHFH\n",
    "        FFFH\n",
    "        HFFG\n",
    "    S : starting point, safe\n",
    "    F : frozen surface, safe\n",
    "    H : hole, fall to your doom\n",
    "    G : goal, where the frisbee is located\n",
    "    \n",
    "    \n",
    "### Action Space\n",
    "The player/agent can take 4 discrete actions, in the range {0,3}\n",
    "* 0: Move left\n",
    "* 1: Move down\n",
    "* 2: Move right \n",
    "* 4: Move up\n",
    "\n",
    "\n",
    "### State Space\n",
    "The environment consists of 16 states. The state is a value representing the playerâ€™s current position as current_row * nrows + current_col (where both the row and col start at 0).\n",
    "For example, the goal position in the 4x4 map can be calculated as follows: 3 * 4 + 3 = 15.\n",
    "\n",
    "\n",
    "### Starting State\n",
    "The episode starts with the player in state [0] (location [0, 0]).\n",
    "\n",
    "\n",
    "### Rewards \n",
    "\n",
    "* Reach goal: +1\n",
    "* Reach hole: 0\n",
    "* Reach frozen: 0\n",
    "\n",
    "### Episode End\n",
    "The episode ends if the following happens:\n",
    "#### 1.Termination:\n",
    "* The player moves into a hole.\n",
    "* The player reaches the goal at max(nrow) * max(ncol) - 1 (location [max(nrow)-1, max(ncol)-1]).\n",
    "\n",
    "#### 2.Truncation:\n",
    "* The length of the episode is 100 for 4x4 environment.\n",
    "\n",
    "For more info refer to source: https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n",
    "\n",
    "### The Environment Parameters\n",
    "* Use discount factor, $\\gamma = 0.9$\n",
    "* The environment is slippery, ie., the transition kernel is stochastic.\n",
    "* The transition kernel P is a dictionary. \n",
    "* P[state][action] is tuples with (probability, nextstate, reward, terminal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the following initializer. Make sure you can execute it without any error.**\n",
    "\n",
    "If you wish to finish this assignment using Google Colab. Uncomment the following commands and run them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install swig\n",
    "# !pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "%matplotlib inline\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "env = gym.make('FrozenLake-v1', desc=None,map_name=\"4x4\", is_slippery=True)\n",
    "while hasattr(env, 'env'):\n",
    "    env = env.env\n",
    "gamma = 0.9\n",
    "\n",
    "\n",
    "# The optimal Q-value function from HW01\n",
    "Q_opt = np.array([[0.06887237, 0.06663045, 0.06663045, 0.05974078],\n",
    "                 [0.03907988, 0.04297989, 0.04073797, 0.06139887],\n",
    "                 [0.07439831, 0.06881719, 0.0727172,  0.05747583],\n",
    "                 [0.03905686, 0.03905686, 0.03347574, 0.05579473],\n",
    "                 [0.09183769, 0.07117679, 0.06428712, 0.04821147],\n",
    "                 [0.,         0.,         0.,         0.        ],\n",
    "                 [0.11220205, 0.08988305, 0.11220205, 0.02231899],\n",
    "                 [0.,         0.,         0.,         0.        ],\n",
    "                 [0.07117679, 0.11787214, 0.1017965,  0.14542271],\n",
    "                 [0.15760471, 0.24748776, 0.20386154, 0.13350927],\n",
    "                 [0.29961112, 0.26595078, 0.22536519, 0.10790627],\n",
    "                 [0.,         0.,         0.,         0.        ],\n",
    "                 [0.,         0.,         0.,         0.        ],\n",
    "                 [0.18822442, 0.30568334, 0.37992926, 0.26595078],\n",
    "                 [0.39556639, 0.63901667, 0.61492124, 0.53719488],\n",
    "                 [0.,         0.,         0.,         0.        ]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tabular Q-Learning\n",
    "\n",
    "### The decay of learning rate and exploration factor\n",
    "We aim to learn the optimal Q-value function through online exploration. Intuitively, after sufficient exploration, our learner should already have good amount of awareness in the scary FrozenLake. Too much exploration results in a sub-optimal policy. Hence, we should dynamically adjust our exploration scheme.\n",
    "\n",
    "The convergence of an online learning algorithm is highly correlated to the stochasticity of the environment. As a result, you may see your Q-value function fluctuates frequently. This could be because the learner just faced a bad episode! However, after training for a while, your learner should have seen sufficient number of both good and bad episodes. Decaying the learning rate can help the algorithm to converge to the optimal policy.\n",
    "\n",
    "### Your Task\n",
    "1. Implement `lr_decay` and `epsilon_decay`. Experiment with linear decay, exponential decay, etc. You do not need to find **THE** best decay scheme.\n",
    "2. Implement `epsilon_greedy'. One way to do this is to sample a Bernoulli random variable with parameter $\\epsilon$. Note that, $\\epsilon$ is the probability that **you choose a random action instead of the greedy action**.\n",
    "3. Implement Q-Learning update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_decay(k):\n",
    "    \"\"\"Return the learning rate for k-th episode\n",
    "\n",
    "    Args:\n",
    "        k: The index of episode.\n",
    "    Returns:\n",
    "        lr: The learning rate for this episode.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    # TASK 1\n",
    "    lr = 0.5\n",
    "    ### END OF YOUR CODE\n",
    "    return lr\n",
    "\n",
    "def epsilon_decay(k):\n",
    "    \"\"\"Return the decayed exploration factor\n",
    "    \n",
    "    Args:\n",
    "        k: The index of episode.\n",
    "    Returns:\n",
    "        epsilon: The exploration factor for this episode.\n",
    "    \"\"\"    \n",
    "    ### YOUR CODE HERE\n",
    "    # TASK 1\n",
    "    epsilon = 0.5\n",
    "    ### END OF YOUR CODE\n",
    "    return epsilon\n",
    "\n",
    "def epsilon_greedy(Q, s, epsilon):\n",
    "    \"\"\"Outputs an action acoording to epsilon-greedy policy\n",
    "\n",
    "    Args:\n",
    "        Q (matrix of |S|x|A|): matrix of Q-functions, where Q(s,a) is at s-th row and a-th column\n",
    "        s (int): current state where next action is inquired upon\n",
    "        epsilon (0<=float<=1): Pr(random exploration and avoid optimal action) = epsilon\n",
    "\n",
    "    Return:\n",
    "        a (int): next action\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    # TASK 2\n",
    "    a = 0\n",
    "    ### END OF YOUR CODE\n",
    "    return int(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_actions = env.action_space.n\n",
    "no_of_states = env.observation_space.n\n",
    "\n",
    "max_step = 100\n",
    "max_episode = 30000\n",
    "\n",
    "Q = np.zeros((no_of_states, no_of_actions))\n",
    "G_list = [] # cumulative discounted reward in episode k\n",
    "convergence_list = [] # record ||Q_k-Q_STAR||_{2}\n",
    "\n",
    "for k in range(max_episode):\n",
    "    s0, _ = env.reset()  # start new episode\n",
    "    path_k = [s0]  # initial state for new episode\n",
    "    G_k = 0  # reset cumulative discounted reward for episode k\n",
    "    for t in range(max_step):\n",
    "        s_t = path_k[-1] # observe current state s_t\n",
    "        a_t = epsilon_greedy(Q, s_t, epsilon_decay(k))  # decide what action to take at step t\n",
    "        ns_t, r_t, terminal, _, _ = env.step(a_t)  # take a step\n",
    "        ### YOUR CODE HERE\n",
    "        # TASK 3\n",
    "        # Hint: you should call lr_decay somewhere here.\n",
    "        # Q[s_t][a_t] = \n",
    "        ### END OF YOUR CODE\n",
    "        # We have updated the following line\n",
    "        G_k += r_t  # update cumulative reward\n",
    "        path_k.append(ns_t)  # record next state\n",
    "        if terminal:  # next state is a terminal state, episode ended\n",
    "            break\n",
    "    convergence_list.append(np.linalg.norm(Q.flatten()-Q_opt.flatten())) # calc and store Euclidean norm of Q_k-Q*\n",
    "    G_list.append(G_k) # record cumulative discounted reward for episode k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Plot $G_k$ using a sliding window\n",
    "\n",
    "$G_k$ is the cumulative reward obtained in episode $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "# Some plot stuff\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Plot $||Q_k - Q^*||$\n",
    "\n",
    "Note that use `Q_opt` for $Q^*$. It is given to you in the previous section of this file. You do not need to copy it from HW1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "# Some plot stuff\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. Question\n",
    "\n",
    "What is the policy and Q-value function obtained at the end of the learning? Are you\n",
    "able to learn the optimal policy? That is, how to get the optimal policy from the optimal Q-value function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "print(Q)\n",
    "print(Q_opt)\n",
    "# Compute the final policy\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answer below.**  \n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Behavior Policy  \n",
    "\n",
    "Implement tabular Q-learning with a uniformly random policy (where\n",
    "each action is taken with equal probability) as the behavior policy. Compare the convergence\n",
    "with the Îµ-greedy exploration approach. Explain your observations and inference. Can you\n",
    "implement a better behavior policy and show its effectiveness?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "# Freestyle!\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answer below.**  \n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TD-Learning\n",
    "\n",
    "Consider the following polices: (i) the optimal policy obtained from QVI,\n",
    "and (ii) a uniformly random policy where each action is taken with equal probability. Learn\n",
    "the value of the these polices using:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Monte Carlo (MC) Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "# Freestyle!\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Temporal Difference (TD) Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "# Freestyle!\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. What are the trade-offs of between MC vs TD?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answer below.**  \n",
    "Answer:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
